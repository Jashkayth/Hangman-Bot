{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29432c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 548064 unique words to 'C:\\Users\\7ashk\\Downloads\\combined_unique.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path=r\"C:\\Users\\7ashk\\Downloads\";\n",
    "file1 = os.path.join(data_path, 'words.txt')\n",
    "file2 = os.path.join(data_path, 'words_250000_train.txt')\n",
    "output_file = os.path.join(data_path, 'combined_unique.txt')\n",
    "\n",
    "\n",
    "for f in (file1, file2):\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Couldn't find {f}\")\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for fname in (file1, file2):\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            if not word or ' ' in word:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                for part in parts:\n",
    "                    if part and ' ' not in part:\n",
    "                        unique_words.add(part)\n",
    "                concat = ''.join(parts)\n",
    "                if concat and ' ' not in concat:\n",
    "                    unique_words.add(concat)\n",
    "            else:\n",
    "                unique_words.add(word)\n",
    "out_dir = os.path.dirname(output_file)\n",
    "if out_dir and not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for w in unique_words:\n",
    "        out.write(w + '\\n')\n",
    "\n",
    "print(f\"Written {len(unique_words)} unique words to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ALPHA_RE=\"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_hyphenated(token: str, *, min_len: int = 2, max_len: int = 25) -> List[str]:\n",
    "    \n",
    "    if \"-\" not in token:\n",
    "        return []\n",
    "\n",
    "    pieces = token.split(\"-\")\n",
    "    out: set[str] = set()\n",
    "\n",
    "    # individual pieces\n",
    "    for p in pieces:\n",
    "        p = p.lower()\n",
    "        if min_len <= len(p) <= max_len and _ALPHA_RE.fullmatch(p):\n",
    "            out.add(p)\n",
    "\n",
    "    # concatenated form\n",
    "    concat = \"\".join(pieces).lower()\n",
    "    if min_len <= len(concat) <= max_len and _ALPHA_RE.fullmatch(concat):\n",
    "        out.add(concat)\n",
    "\n",
    "    return list(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_words(\n",
    "    path: str | Path,\n",
    "    *,\n",
    "    min_len: int = 2,\n",
    "    max_len: int = 25,\n",
    "    sample: int = 20,\n",
    ") -> Tuple[list[str], list[str]]:\n",
    "    path = Path(path)\n",
    "\n",
    "    clean: list[str] = []\n",
    "    rejected: list[str] = []\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for raw in fh:\n",
    "            raw = raw.strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "\n",
    "            raw_low = raw.lower()\n",
    "\n",
    "            if \"-\" in raw_low:\n",
    "                # Expand and record original as rejected\n",
    "                expanded = expand_hyphenated(raw_low, min_len=min_len, max_len=max_len)\n",
    "                clean.extend(expanded)\n",
    "                rejected.append(raw_low)\n",
    "                continue\n",
    "\n",
    "            if min_len <= len(raw_low) <= max_len and _ALPHA_RE.fullmatch(raw_low):\n",
    "                clean.append(raw_low)\n",
    "            else:\n",
    "                rejected.append(raw_low)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    clean_unique = list(dict.fromkeys(clean))\n",
    "    rejected_unique = list(dict.fromkeys(rejected))\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {len(clean_unique):,} clean words (\" f\"{len(rejected_unique):,} rejected)\"\n",
    "    )\n",
    "    if sample and rejected_unique:\n",
    "        print(\"Sample rejected:\", rejected_unique[:sample])\n",
    "\n",
    "    return clean_unique, rejected_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fc058f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_dataframe(words: Iterable[str]) -> \"pd.DataFrame\":\n",
    "    df = pd.DataFrame({\"word\": list(words)})\n",
    "    df[\"len\"] = df.word.str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe26f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "068feed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 456,198 clean words (50,664 rejected)\n",
      "Sample rejected: ['spoon-fed', 'dim-brooding', 'gospel-true', 'tub-t', \"coyote's\", 'half-grecized', 'almain-rivets', 'stout-bodied', 'co.', 'uncross-examined', \"haunch's\", 'pilot-boat', 'slow-breathing', 'try-', 'snaky-tailed', 'ill-friended', 'grim-looking', 'self-collectedness', 'franco-provencal', 'second-guess']\n"
     ]
    }
   ],
   "source": [
    "words, rejected = load_clean_words(txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_to_file(words: Iterable[str], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for w in words:\n",
    "            f.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a383a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 456,198 clean words (50,664 rejected)\n",
      "Sample rejected: ['spoon-fed', 'dim-brooding', 'gospel-true', 'tub-t', \"coyote's\", 'half-grecized', 'almain-rivets', 'stout-bodied', 'co.', 'uncross-examined', \"haunch's\", 'pilot-boat', 'slow-breathing', 'try-', 'snaky-tailed', 'ill-friended', 'grim-looking', 'self-collectedness', 'franco-provencal', 'second-guess']\n"
     ]
    }
   ],
   "source": [
    "words, _ = load_clean_words(txt_path)\n",
    "save_words_to_file(words, r\"C:\\Users\\7ashk\\Downloads\\final_words.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
